{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e60ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import cv2\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import sklearn.preprocessing\n",
    "import os\n",
    "import keras\n",
    "import scipy.io as sio\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import scipy\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a1dd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "C3D_MEAN_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/c3d_mean.npy'\n",
    "WEIGHTS_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/sports1M_weights_tf.h5'\n",
    "\n",
    "\n",
    "def c3d_preprocess_input(video):\n",
    "    \"\"\"Preprocess video input to make it suitable for feature extraction.\n",
    "    The video is resized, cropped, resampled and training mean is substracted\n",
    "    to make it suitable for the network\n",
    "    :param video: Video to be processed\n",
    "    :returns: Preprocessed video\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    intervals = np.ceil(np.linspace(0, video.shape[0] - 1, 16)).astype(int)\n",
    "    frames = video[intervals]\n",
    "\n",
    "    # Reshape to 128x171\n",
    "    reshape_frames = np.zeros((frames.shape[0], 128, 171, frames.shape[3]))\n",
    "    for i, img in enumerate(frames):\n",
    "        img = cv2.resize(src=img, dsize=(171,128), interpolation=cv2.INTER_CUBIC)\n",
    "        reshape_frames[i,:,:,:] = img\n",
    "\n",
    "\n",
    "    mean_path = get_file('c3d_mean.npy',\n",
    "                         C3D_MEAN_PATH,\n",
    "                         cache_subdir='models',\n",
    "                         md5_hash='08a07d9761e76097985124d9e8b2fe34')\n",
    "\n",
    "    mean = np.load(mean_path)\n",
    "    reshape_frames -= mean\n",
    "    # Crop to 112x112\n",
    "    reshape_frames = reshape_frames[:, 8:120, 30:142, :]\n",
    "    # Add extra dimension for samples\n",
    "    reshape_frames = np.expand_dims(reshape_frames, axis=0)\n",
    "\n",
    "    return reshape_frames\n",
    "\n",
    "\n",
    "def C3D(weights='sports1M'):\n",
    "    \"\"\"Creation of the full C3D architecture\n",
    "    :param weights: Weights to be loaded into the network. If None,\n",
    "    the network is randomly initialized.\n",
    "    :returns: Network model\n",
    "    :rtype: keras.model\n",
    "    \"\"\"\n",
    "\n",
    "    if weights not in {'sports1M', None}:\n",
    "        raise ValueError('weights should be either be sports1M or None')\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        shape = (16, 112, 112, 3)\n",
    "    else:\n",
    "        shape = (3, 16, 112, 112)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', name='conv1',\n",
    "                                            input_shape=shape),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(1,2,2), strides=(1,2,2),\n",
    "                                            padding='same', name='pool1'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', name='conv2'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='valid',\n",
    "                                            name='pool2'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same',name='conv3a'),\n",
    "        tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same',name='conv3b'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2),\n",
    "                                            padding='valid', name='pool3'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same',name='conv4a'),\n",
    "        tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same',name='conv4b'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2),\n",
    "                                            padding='valid', name='pool4'),\n",
    "        \n",
    "        tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same',name='conv5a'),\n",
    "        tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same',name='conv5b'),\n",
    "        tf.keras.layers.ZeroPadding3D(padding=(0,1,1)),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2),\n",
    "                                            padding='valid', name='pool5'),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        tf.keras.layers.Dense(4096, activation='relu', name='fc6'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(4096, activation='relu', name='fc7'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(487, activation='softmax', name='fc8'),\n",
    "    ])\n",
    "    \n",
    "    if weights == 'sports1M':\n",
    "        weights_path = get_file('sports1M_weights_tf.h5',\n",
    "                                WEIGHTS_PATH,\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash='b7a93b2f9156ccbebe3ca24b41fc5402')\n",
    "        model.load_weights(weights_path)\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def c3d_feature_extractor():\n",
    "    \"\"\"Creation of the feature extraction architecture. This network is\n",
    "    formed by a subset of the original C3D architecture (from the\n",
    "    beginning to fc6 layer)\n",
    "    :returns: Feature extraction model\n",
    "    :rtype: keras.model\n",
    "    \"\"\"\n",
    "    base_model = C3D(weights='sports1M')\n",
    "    layer_name = 'fc6'\n",
    "    #model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc6').output)\n",
    "    feature_extractor_model = Model(inputs=base_model.input,outputs=base_model.get_layer(layer_name).output)\n",
    "    return feature_extractor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8caa97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model():\n",
    "    \"\"\"Build the classifier\n",
    "    :returns: Classifier model\n",
    "    :rtype: keras.Model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=4096, kernel_initializer='glorot_normal',\n",
    "                    kernel_regularizer=l2(0.001), activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(32, kernel_initializer='glorot_normal',\n",
    "                    kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(1, kernel_initializer='glorot_normal',\n",
    "                    kernel_regularizer=l2(0.001), activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feacac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_dict(dict2):\n",
    "    \"\"\"Prepare the dictionary of weights to be loaded by the network\n",
    "    :param dict2: Dictionary to format\n",
    "    :returns: The dictionary properly formatted\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    for i in range(len(dict2)):\n",
    "        if str(i) in dict2:\n",
    "            if dict2[str(i)].shape == (0, 0):\n",
    "                dict[str(i)] = dict2[str(i)]\n",
    "            else:\n",
    "                weights = dict2[str(i)][0]\n",
    "                weights2 = []\n",
    "                for weight in weights:\n",
    "                    if weight.shape in [(1, x) for x in range(0, 5000)]:\n",
    "                        weights2.append(weight[0])\n",
    "                    else:\n",
    "                        weights2.append(weight)\n",
    "                dict[str(i)] = weights2\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87fd86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights_file):\n",
    "    \"\"\"Loads the pretrained weights into the network architecture\n",
    "    :param model: keras model of the network\n",
    "    :param weights_file: Path to the weights file\n",
    "    :returns: The input model with the weights properly loaded\n",
    "    :rtype: keras.model\n",
    "    \"\"\"\n",
    "    dict2 = sio.loadmat(weights_file)\n",
    "    dict = conv_dict(dict2)\n",
    "    i = 0\n",
    "    for layer in model.layers:\n",
    "        weights = dict[str(i)]\n",
    "        layer.set_weights(weights)\n",
    "        i += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9cc8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier and load pretrained weights\n",
    "def create_classifier_model():\n",
    "    model = classifier_model()\n",
    "    model = load_weights(model, './weights_L1L2.mat')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8ae28",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c574907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_height = 240\n",
    "frame_width = 320\n",
    "channels = 3\n",
    "\n",
    "frame_count = 16\n",
    "\n",
    "features_per_bag = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8047cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './'\n",
    "sample_video_path = 'Anomaly-Videos-Part-4/Stealing/Stealing003_x264.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "794b74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clip(clip, convert_bgr=False, save_gif=False, file_path=None):\n",
    "    num_frames = len(clip)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    def update(i):\n",
    "        if convert_bgr:\n",
    "            frame = cv2.cvtColor(clip[i], cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            frame = clip[i]\n",
    "        plt.imshow(frame)\n",
    "        return plt\n",
    "\n",
    "    # FuncAnimation will call the 'update' function for each frame; here\n",
    "    # animating over 10 frames, with an interval of 20ms between frames.\n",
    "    anim = FuncAnimation(fig, update, frames=np.arange(0, num_frames), interval=1)\n",
    "    if save_gif:\n",
    "        anim.save(file_path, dpi=80, writer='imagemagick')\n",
    "    else:\n",
    "        # plt.show() will just loop the animation forever.\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(video_path, predictions, save_path):\n",
    "    frames = get_video_frames(video_path)\n",
    "    assert len(frames) == len(predictions)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    line = matplotlib.lines.Line2D([], [])\n",
    "\n",
    "    fig_frame = plt.subplot(2, 1, 1)\n",
    "    img = fig_frame.imshow(frames[0])\n",
    "    fig_prediction = plt.subplot(2, 1, 2)\n",
    "    fig_prediction.set_xlim(0, len(frames))\n",
    "    fig_prediction.set_ylim(0, 1.15)\n",
    "    fig_prediction.add_line(line)\n",
    "\n",
    "    def update(i):\n",
    "        frame = frames[i]\n",
    "        x = range(0, i)\n",
    "        y = predictions[0:i]\n",
    "        line.set_data(x, y)\n",
    "        img.set_data(frame)\n",
    "        return plt\n",
    "\n",
    "    # FuncAnimation will call the 'update' function for each frame; here\n",
    "    # animating over 10 frames, with an interval of 20ms between frames.\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=np.arange(0, len(frames), 10), interval=1, repeat=False)\n",
    "\n",
    "    if save_path:\n",
    "        anim.save(save_path, dpi=200, writer='imagemagick')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a14b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(arr, size, stride):\n",
    "    \"\"\"Apply sliding window to an array, getting chunks of\n",
    "    of specified size using the specified stride\n",
    "    :param arr: Array to be divided\n",
    "    :param size: Size of the chunks\n",
    "    :param stride: Number of frames to skip for the next chunk\n",
    "    :returns: Tensor with the resulting chunks\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    num_chunks = int((len(arr) - size) / stride) + 2\n",
    "    result = []\n",
    "    for i in range(0,  num_chunks * stride, stride):\n",
    "        if len(arr[i:i + size]) > 0:\n",
    "            result.append(arr[i:i + size])\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def interpolate(features, features_per_bag):\n",
    "    \"\"\"Transform a bag with an arbitrary number of features into a bag\n",
    "    with a fixed amount, using interpolation of consecutive features\n",
    "    :param features: Bag of features to pad\n",
    "    :param features_per_bag: Number of features to obtain\n",
    "    :returns: Interpolated features\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    feature_size = np.array(features).shape[1]\n",
    "    interpolated_features = np.zeros((features_per_bag, feature_size))\n",
    "    interpolation_indices = np.round(np.linspace(0, len(features) - 1, num=features_per_bag + 1))\n",
    "    count = 0\n",
    "    for index in range(0, len(interpolation_indices)-1):\n",
    "        start = int(interpolation_indices[index])\n",
    "        end = int(interpolation_indices[index + 1])\n",
    "\n",
    "        assert end >= start\n",
    "\n",
    "        if start == end:\n",
    "            temp_vect = features[start, :]\n",
    "        else:\n",
    "            temp_vect = np.mean(features[start:end+1, :], axis=0)\n",
    "\n",
    "        temp_vect = temp_vect / np.linalg.norm(temp_vect)\n",
    "\n",
    "        if np.linalg.norm(temp_vect) == 0:\n",
    "            print(\"Error\")\n",
    "\n",
    "        interpolated_features[count,:]=temp_vect\n",
    "        count = count + 1\n",
    "\n",
    "    return np.array(interpolated_features)\n",
    "\n",
    "\n",
    "def extrapolate(outputs, num_frames):\n",
    "    \"\"\"Expand output to match the video length\n",
    "    :param outputs: Array of predicted outputs\n",
    "    :param num_frames: Expected size of the output array\n",
    "    :returns: Array of output size\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    extrapolated_outputs = []\n",
    "    extrapolation_indices = np.round(np.linspace(0, len(outputs) - 1, num=num_frames))\n",
    "    for index in extrapolation_indices:\n",
    "        extrapolated_outputs.append(outputs[int(index)])\n",
    "    return np.array(extrapolated_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad281b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(video_path):\n",
    "    \"\"\"Reads the video given a file path\n",
    "    :param video_path: Path to the video\n",
    "    :returns: Video as an array of frames\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def get_video_clips(video_path):\n",
    "    \"\"\"Divides the input video into non-overlapping clips\n",
    "    :param video_path: Path to the video\n",
    "    :returns: Array with the fragments of video\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    frames = get_video_frames(video_path)\n",
    "    clips = sliding_window(frames, frame_count, frame_count)\n",
    "    return clips, len(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d1c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clips in the video :  224\n",
      "Models initialized\n",
      "Processed clip :  0\n",
      "Processed clip :  1\n",
      "Processed clip :  2\n",
      "Processed clip :  3\n",
      "Processed clip :  4\n",
      "Processed clip :  5\n",
      "Processed clip :  6\n",
      "Processed clip :  7\n",
      "Processed clip :  8\n",
      "Processed clip :  9\n",
      "Processed clip :  10\n",
      "Processed clip :  11\n",
      "Processed clip :  12\n",
      "Processed clip :  13\n",
      "Processed clip :  14\n",
      "Processed clip :  15\n",
      "Processed clip :  16\n",
      "Processed clip :  17\n",
      "Processed clip :  18\n",
      "Processed clip :  19\n",
      "Processed clip :  20\n",
      "Processed clip :  21\n",
      "Processed clip :  22\n",
      "Processed clip :  23\n",
      "Processed clip :  24\n",
      "Processed clip :  25\n",
      "Processed clip :  26\n",
      "Processed clip :  27\n",
      "Processed clip :  28\n",
      "Processed clip :  29\n",
      "Processed clip :  30\n",
      "Processed clip :  31\n",
      "Processed clip :  32\n",
      "Processed clip :  33\n",
      "Processed clip :  34\n",
      "Processed clip :  35\n",
      "Processed clip :  36\n",
      "Processed clip :  37\n",
      "Processed clip :  38\n",
      "Processed clip :  39\n",
      "Processed clip :  40\n",
      "Processed clip :  41\n",
      "Processed clip :  42\n",
      "Processed clip :  43\n",
      "Processed clip :  44\n",
      "Processed clip :  45\n",
      "Processed clip :  46\n",
      "Processed clip :  47\n",
      "Processed clip :  48\n",
      "Processed clip :  49\n",
      "Processed clip :  50\n",
      "Processed clip :  51\n",
      "Processed clip :  52\n",
      "Processed clip :  53\n",
      "Processed clip :  54\n",
      "Processed clip :  55\n",
      "Processed clip :  56\n",
      "Processed clip :  57\n",
      "Processed clip :  58\n",
      "Processed clip :  59\n",
      "Processed clip :  60\n",
      "Processed clip :  61\n",
      "Processed clip :  62\n",
      "Processed clip :  63\n",
      "Processed clip :  64\n",
      "Processed clip :  65\n",
      "Processed clip :  66\n",
      "Processed clip :  67\n",
      "Processed clip :  68\n",
      "Processed clip :  69\n",
      "Processed clip :  70\n",
      "Processed clip :  71\n",
      "Processed clip :  72\n",
      "Processed clip :  73\n",
      "Processed clip :  74\n",
      "Processed clip :  75\n",
      "Processed clip :  76\n",
      "Processed clip :  77\n",
      "Processed clip :  78\n",
      "Processed clip :  79\n",
      "Processed clip :  80\n",
      "Processed clip :  81\n",
      "Processed clip :  82\n",
      "Processed clip :  83\n",
      "Processed clip :  84\n",
      "Processed clip :  85\n",
      "Processed clip :  86\n",
      "Processed clip :  87\n",
      "Processed clip :  88\n",
      "Processed clip :  89\n",
      "Processed clip :  90\n",
      "Processed clip :  91\n",
      "Processed clip :  92\n",
      "Processed clip :  93\n",
      "Processed clip :  94\n",
      "Processed clip :  95\n",
      "Processed clip :  96\n",
      "Processed clip :  97\n",
      "Processed clip :  98\n",
      "Processed clip :  99\n",
      "Processed clip :  100\n",
      "Processed clip :  101\n",
      "Processed clip :  102\n",
      "Processed clip :  103\n",
      "Processed clip :  104\n",
      "Processed clip :  105\n",
      "Processed clip :  106\n",
      "Processed clip :  107\n",
      "Processed clip :  108\n",
      "Processed clip :  109\n",
      "Processed clip :  110\n",
      "Processed clip :  111\n",
      "Processed clip :  112\n",
      "Processed clip :  113\n",
      "Processed clip :  114\n",
      "Processed clip :  115\n",
      "Processed clip :  116\n",
      "Processed clip :  117\n",
      "Processed clip :  118\n",
      "Processed clip :  119\n",
      "Processed clip :  120\n",
      "Processed clip :  121\n",
      "Processed clip :  122\n",
      "Processed clip :  123\n",
      "Processed clip :  124\n",
      "Processed clip :  125\n",
      "Processed clip :  126\n",
      "Processed clip :  127\n",
      "Processed clip :  128\n",
      "Processed clip :  129\n",
      "Processed clip :  130\n",
      "Processed clip :  131\n",
      "Processed clip :  132\n",
      "Processed clip :  133\n",
      "Processed clip :  134\n",
      "Processed clip :  135\n",
      "Processed clip :  136\n",
      "Processed clip :  137\n",
      "Processed clip :  138\n",
      "Processed clip :  139\n",
      "Processed clip :  140\n",
      "Processed clip :  141\n",
      "Processed clip :  142\n",
      "Processed clip :  143\n",
      "Processed clip :  144\n",
      "Processed clip :  145\n",
      "Processed clip :  146\n",
      "Processed clip :  147\n",
      "Processed clip :  148\n",
      "Processed clip :  149\n",
      "Processed clip :  150\n",
      "Processed clip :  151\n",
      "Processed clip :  152\n",
      "Processed clip :  153\n",
      "Processed clip :  154\n",
      "Processed clip :  155\n",
      "Processed clip :  156\n",
      "Processed clip :  157\n",
      "Processed clip :  158\n",
      "Processed clip :  159\n",
      "Processed clip :  160\n",
      "Processed clip :  161\n",
      "Processed clip :  162\n",
      "Processed clip :  163\n",
      "Processed clip :  164\n",
      "Processed clip :  165\n",
      "Processed clip :  166\n",
      "Processed clip :  167\n",
      "Processed clip :  168\n",
      "Processed clip :  169\n",
      "Processed clip :  170\n",
      "Processed clip :  171\n",
      "Processed clip :  172\n",
      "Processed clip :  173\n",
      "Processed clip :  174\n",
      "Processed clip :  175\n",
      "Processed clip :  176\n",
      "Processed clip :  177\n",
      "Processed clip :  178\n",
      "Processed clip :  179\n",
      "Processed clip :  180\n",
      "Processed clip :  181\n",
      "Processed clip :  182\n",
      "Processed clip :  183\n",
      "Processed clip :  184\n",
      "Processed clip :  185\n",
      "Processed clip :  186\n",
      "Processed clip :  187\n",
      "Processed clip :  188\n",
      "Processed clip :  189\n",
      "Processed clip :  190\n",
      "Processed clip :  191\n",
      "Processed clip :  192\n",
      "Processed clip :  193\n",
      "Processed clip :  194\n",
      "Processed clip :  195\n",
      "Processed clip :  196\n",
      "Processed clip :  197\n",
      "Processed clip :  198\n",
      "Processed clip :  199\n",
      "Processed clip :  200\n",
      "Processed clip :  201\n",
      "Processed clip :  202\n",
      "Processed clip :  203\n",
      "Processed clip :  204\n",
      "Processed clip :  205\n",
      "Processed clip :  206\n",
      "Processed clip :  207\n",
      "Processed clip :  208\n",
      "Processed clip :  209\n",
      "Processed clip :  210\n",
      "Processed clip :  211\n",
      "Processed clip :  212\n",
      "Processed clip :  213\n",
      "Processed clip :  214\n",
      "Processed clip :  215\n",
      "Processed clip :  216\n",
      "Processed clip :  217\n",
      "Processed clip :  218\n",
      "Processed clip :  219\n",
      "Processed clip :  220\n",
      "Processed clip :  221\n",
      "Processed clip :  222\n",
      "Executed Successfully - Stealing003_x264.gif saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    }
   ],
   "source": [
    "def run_demo():\n",
    "\n",
    "    video_name = os.path.basename(sample_video_path).split('.')[0]\n",
    "\n",
    "    # read video\n",
    "    video_clips, num_frames = get_video_clips(sample_video_path)\n",
    "\n",
    "    print(\"Number of clips in the video : \", len(video_clips))\n",
    "\n",
    "    # build models\n",
    "    feature_extractor = c3d_feature_extractor()\n",
    "    classifier_model = create_classifier_model()\n",
    "\n",
    "    print(\"Models initialized\")\n",
    "\n",
    "    # extract features\n",
    "    rgb_features = []\n",
    "    for i, clip in enumerate(video_clips):\n",
    "        clip = np.array(clip)\n",
    "        if len(clip) < frame_count:\n",
    "            continue\n",
    "\n",
    "        clip = c3d_preprocess_input(clip)\n",
    "        rgb_feature = feature_extractor.predict(clip)[0]\n",
    "        rgb_features.append(rgb_feature)\n",
    "\n",
    "        print(\"Processed clip : \", i)\n",
    "\n",
    "    rgb_features = np.array(rgb_features)\n",
    "    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n",
    "    \n",
    "    # classify using the trained classifier model\n",
    "    predictions = classifier_model.predict(rgb_feature_bag)\n",
    "\n",
    "    predictions = np.array(predictions).squeeze()\n",
    "\n",
    "    predictions = extrapolate(predictions, num_frames)\n",
    "    \n",
    "    save_path = os.path.join('./', video_name + '.gif')\n",
    "    # visualize predictions\n",
    "    print('Executed Successfully - '+video_name + '.gif saved')\n",
    "    visualize_predictions(sample_video_path, predictions, save_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03144145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
